<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AMKG: Adaptive Multimodal Knowledge Graph Integration for Object-Goal Navigation in Open-World Embodied Systems</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            margin: 20px auto;
            background-color: #f8f9fa;
            max-width: 1000px;  /* 页面最大宽度 */
            padding: 0 20px;  /* 左右内边距，防止内容太靠边 */
        }
        h1 {
            font-size: 28px;
            font-weight: bold;
            margin-bottom: 20px; /* 调整标题下方的间距 */
        }
        h2 {
            font-size: 24px;
            margin-top: 30px; /* 调整小标题上方间距 */
            margin-bottom: 15px; /* 调整小标题下方间距 */
        }
        .authors {
            font-size: 16px;
            color: #007bff;
        }
        .affiliations {
            font-size: 14px;
            color: #555;
        }
        .button-container {
            text-align: center;
            margin-top: 20px;
        }
        .button {
            display: inline-block;
            padding: 12px 25px;
            margin: 0 10px;
            text-decoration: none;
            font-weight: bold;
            color: white;
            border-radius: 30px;
            transition: background-color 0.3s;
        }
        .pdf-button {
            background-color: #007bff; /* 蓝色 */
        }
        .code-button {
            background-color: #6c757d; /* 灰色 */
        }
        .arxiv-button {
            background-color: #6c757d; /* 灰色 */
        }
        .button:hover {
            background-color: #0056b3;
        }
        .button:active {
            background-color: #004085;
        }

        .video-container {
            margin: 20px auto;
            width: 80%;
            max-width: 800px;
        }
        video {
            width: 100%;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }

        .highlights-section {
            background-color: #e0e0e0;
            padding: 20px;
            border-radius: 10px;
            margin: 20px auto;
            width: 80%;
        }

        .video-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 15px;
            justify-content: center;
            margin: 20px auto;
            width: 80%;
        }
        .video-grid video {
            width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
        }

        .image-description-container {
            background-color: #f8f9fa;
            padding: 20px;
            margin: 20px auto;
            width: 100%;
            max-width: 800px;
            border-radius: 10px;
            box-shadow: none;
            text-align: center;
        }

        .image-container {
            width: 100%;
        }

        .image-container img {
            width: 100%;
            height: auto;
            object-fit: contain;
            max-height: 500px;
            border-radius: 10px;
        }

        .description-section {
            width: 100%;
            font-size: 16px;
            color: #333;
            line-height: 1.6;
            margin-top: 15px;
            padding: 0 10px;
        }

        @media (max-width: 768px) {
            .image-description-container {
                padding: 15px;
                max-width: 100%;
            }
            .description-section {
                font-size: 14px;
            }
        }
    </style>
</head>
<body>

    <h1>AMKG: Adaptive Multimodal Knowledge Graph Integration for Object-Goal Navigation in Open-World Embodied Systems</h1>
    <p class="authors">
        Mingyi Li, Jiaying Li, Hongxiao Wu, Jiawei Li <br>
    </p>
    <p class="affiliations">
        BIT, XMU, HTT, IMUFE <br>
        *Equal Contribution, ordered alphabetically | †Equal Advising
    </p>

    <div class="button-container">
        <a href="doc/AMKG.pdf" class="button pdf-button" title="PDF available">pdf</a>
        <a href="#" class="button code-button disabled" title="Code (Coming Soon)" style="pointer-events: none; background-color: #dcdcdc;">Code</a>
        <a href="#" class="button arxiv-button disabled" title="arXiv (Coming Soon)" style="pointer-events: none; background-color: #dcdcdc;">arxiv</a>
    </div>

    <div class="video-container">
        <video controls>
            <source src="mp4/01.mp4" type="video/mp4">
            <source src="video.webm" type="video/webm">
            您的浏览器不支持视频标签，请使用现代浏览器查看视频。
        </video>
    </div>

    <div class="highlights-section">
        <h2>Highlights</h2>
        <p>
            <strong>High-Precision Multimodal Retrieval with Dynamic Knowledge Graphs:</strong>  
            We introduce an EMKG system that employs Dynamic Graph Neural Networks (DGNN) to improve the multimodal retrieval accuracy and mean Average Precision (mAP), achieving a 12.3% improvement in retrieval accuracy and a 4.92% increase in mAP across various 3D scene datasets. This system demonstrates effective generalization in zero-shot tasks within complex 3D environments.
        </p>
        <p>
            <strong>EMKG-VLM Framework for Object Goal Navigation:</strong>  
            The EMKG-VLM framework enhances object goal navigation by integrating real-world data into the Habitat simulator, thus improving decision-making and retrieval precision in unknown environments and exhibiting high adaptability in simulated navigation.
        </p>
        <p>
            <strong>Real-Time Autonomous Actions through Optimized Vision-Language Commands and Reinforcement Learning:</strong>  
            We combine mobile-optimized InternLM2 visual-language commands with PPO-Clip reinforcement learning to enable real-time decision-making and autonomous navigation in dynamic open-world environments, thus improving adaptability and effectiveness for real-world applications.
        </p>
    </div>

    <h2>Simulation Experiment</h2>
    <div class="video-grid">
        <video controls><source src="mp4/03-1.mp4" type="video/mp4"></video>
        <video controls><source src="mp4/03-2.mp4" type="video/mp4"></video>
        <video controls><source src="mp4/03-3.mp4" type="video/mp4"></video>
        <video controls><source src="mp4/03-4.mp4" type="video/mp4"></video>
        <video controls><source src="mp4/03-5.mp4" type="video/mp4"></video>
        <video controls><source src="mp4/03-6.mp4" type="video/mp4"></video>
        <video controls><source src="mp4/03-7.mp4" type="video/mp4"></video>
        <video controls><source src="mp4/03-8.mp4" type="video/mp4"></video>
    </div>

    <h2>Real-World Experiment 1: The designated 203 meeting room was found through the information in the knowledge base.</h2>
    <div class="video-container">
        <video controls>
            <source src="mp4/02-1.mp4" type="video/mp4">
            <source src="model_demo.webm" type="video/webm">
            您的浏览器不支持视频标签，请使用现代浏览器查看视频。
        </video>
    </div>

    <h2>Real-World Experiment 2: Searching for purple school bag hanging on white table with knowledge base information.</h2>
    <div class="video-container">
        <video controls>
            <source src="mp4/02-2.mp4" type="video/mp4">
            <source src="model_demo.webm" type="video/webm">
            您的浏览器不支持视频标签，请使用现代浏览器查看视频。
        </video>
    </div>

    <div class="image-description-container">
        <div class="image-container">
            <img src="img/001.jpg" alt="Visualization Image">
        </div>
        <div class="description-section">
            <p>
                The EMKG-VLM system integrates 3D point cloud processing with multimodal knowledge graphs and a vision-language model, enabling autonomous robotic navigation and task execution through real-time decision-making and spatial reasoning. The system processes input data, generates an embodied action plan, and selects actions for real-world interaction.
            </p>
        </div>
    </div>

    <div class="image-description-container">
        <div class="image-container">
            <img src="img/002.jpg" alt="Visualization Image">
        </div>
        <div class="description-section">
            <p>
                The system processes navigation tasks by first categorizing the environment with the MRAG Graph Vector (Step 1), followed by Retrieval (Step 2) to gather relevant data. The Generate Goal step (Step 3) then creates navigation goals, such as locating objects at specific coordinates. Finally, Embodied Planning (Step 4) uses MobileVLM for real-time decision-making, enabling efficient navigation and task execution.
            </p>
        </div>
    </div>
	
	<!-- 底部区域 -->
	<footer style="background-color: #343a40; color: #fff; padding: 20px; text-align: center;">
	
		<p>
			<a href="#top" style="color: #ffc107; text-decoration: none;">Back to Top</a>
		</p>
	</footer>


</body>
</html>
